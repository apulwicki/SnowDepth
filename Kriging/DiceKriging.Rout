
R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> ############################################################################
> #Dice Kriging
> ############################################################################
> 
> ## Libraries
> library(R.matlab)
R.matlab v3.6.1 (2016-10-19) successfully loaded. See ?R.matlab for help.

Attaching package: ‘R.matlab’

The following objects are masked from ‘package:base’:

    getOption, isOpen

> library(DiceKriging)
> library(DiceOptim)
> library(foreach)
> 
> 
> ## Load my data ##
> #residuals = readMat('/Users/Alexandra/Documents/SFU/Data/SnowDepth/Kriging/residuals.mat')
> residuals = readMat('/home/glaciology1/Documents/Data/SnowDepth/Kriging/residuals.mat')
> res = residuals$res
> utm = data.frame(residuals$utm)
> sizexy = residuals$sizexy
> 
> ## Model ##
> m = km(~1,design = utm, response = res, covtype = "matern5_2", nugget.estim = TRUE, multistart = 5, iso = TRUE)

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : unknown homogenous nugget effect 
  - parameters lower bounds :  1e-10 
  - parameters upper bounds :  4501.489 
  - upper bound for alpha   :  1 
  - best initial criterion value(s) :  88.54114 85.21202 84.58982 84.29234 82.32933 

N = 2, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -88.541  |proj g|=     0.031902
At iterate     1  f =      -88.889  |proj g|=      0.021135
At iterate     2  f =      -88.912  |proj g|=      0.018563
At iterate     3  f =      -88.912  |proj g|=       0.25971
At iterate     4  f =      -88.912  |proj g|=         0.003
At iterate     5  f =      -88.912  |proj g|=     0.0020111

iterations 5
function evaluations 8
segments explored during Cauchy searches 6
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 0.0020111
final function value -88.9118

F = -88.9118
final  value -88.911783 
converged
N = 2, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -85.212  |proj g|=     0.084505
At iterate     1  f =      -88.552  |proj g|=      0.018443
At iterate     2  f =      -88.619  |proj g|=       0.98774
At iterate     3  f =      -88.635  |proj g|=      0.014824
At iterate     4  f =      -88.638  |proj g|=      0.014132
At iterate     5  f =      -88.638  |proj g|=       0.15338
At iterate     6  f =      -88.638  |proj g|=      0.002328
At iterate     7  f =      -88.638  |proj g|=     0.0023279

iterations 7
function evaluations 11
segments explored during Cauchy searches 9
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 0.00232793
final function value -88.6384

F = -88.6384
final  value -88.638356 
converged
N = 2, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       -84.59  |proj g|=       0.0509
At iterate     1  f =      -87.794  |proj g|=     0.0075517
At iterate     2  f =      -87.802  |proj g|=      0.006938
At iterate     3  f =      -87.802  |proj g|=       0.38853
At iterate     4  f =      -87.802  |proj g|=     0.0047616
At iterate     5  f =      -87.802  |proj g|=     0.0025537

iterations 5
function evaluations 12
segments explored during Cauchy searches 7
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 0.0025537
final function value -87.8022

F = -87.8022
final  value -87.802217 
converged
N = 2, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -84.292  |proj g|=     0.030814
At iterate     1  f =      -87.132  |proj g|=     0.0045548
At iterate     2  f =      -87.141  |proj g|=     0.0041721
At iterate     3  f =      -87.141  |proj g|=       0.67893
At iterate     4  f =      -87.141  |proj g|=     0.0041354
At iterate     5  f =      -87.141  |proj g|=     0.0024712

iterations 5
function evaluations 12
segments explored during Cauchy searches 7
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 0.00247119
final function value -87.1409

F = -87.1409
final  value -87.140903 
converged
N = 2, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -82.329  |proj g|=      0.23172
At iterate     1  f =      -90.894  |proj g|=      0.016699
At iterate     2  f =      -90.936  |proj g|=      0.014685
At iterate     3  f =      -90.936  |proj g|=       0.90941
At iterate     4  f =      -90.936  |proj g|=      0.014559
At iterate     5  f =      -90.936  |proj g|=      0.014558

iterations 5
function evaluations 11
segments explored during Cauchy searches 7
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 0.0145577
final function value -90.9364

F = -90.9364
final  value -90.936433 
converged

* The 5 best values (multistart) obtained are:
 -88.91178 -88.63836 -87.80222 -87.1409 -90.93643 
* The model corresponding to the best one (-90.93643) is stored. 
Warning message:
executing %dopar% sequentially: no parallel backend registered 
>  
>  #plot(m)
>  #m
>  
>  #Return model paramaters
>  maxLL = -m@logLik
>  intercept = m@trend.coef
>  nugget = m@covariance@nugget
>  theta = m@covariance@range.val
>     model = data.frame(intercept, nugget, maxLL, theta)
> 
>  #Cross validation (leave one out)
>  LOO = leaveOneOut.km(m, "SK",trend.reestim = TRUE)
>  
>  
> ## Kriging prediction surface ##
> x = seq(from = 0, to = (sizexy[1,2]-1)*40, by = 40)
> y = seq(from = 0, to = (sizexy[1,1]-1)*40, by = 40)
> grid = expand.grid(X1=x, X2 = y)
> pred.m = predict(m,grid,"SK")
